---
permalink: /
title: "About"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

안녕하세요! 저는 **NLP 연구자이자 LLM 엔지니어**인 유대곤입니다.

## 현재 관심 분야

**검색·임베딩·LLM Post-Training**에 집중하고 있습니다. 특히 효율적인 학습 방법론과 실용적인 머신러닝 파이프라인 구축에 관심이 많습니다.

## 기술 스택

- **Programming**: Python, PyTorch, Transformers
- **ML/NLP**: LangChain, 모델 평가 기술
- **현재 연구**: LLM 엔지니어링 (RLHF, reasoning)

## 경력

**현재 (2025~)**: LLM 엔지니어링 (RLHF, reasoning)
**이전 (2022~2025)**: NLP 연구 (검색 및 도메인 적응)

## 오픈소스 활동

### HuggingFace 기여
- **MTEB-ko-retrieval Leaderboard** 종합 SOTA 성능 모델 공개
- **AutoRAG Embedding Benchmark**에서 Bi-Encoder와 Cross-Encoder 모두 SOTA 달성
- 총 3개의 SOTA 모델을 [HuggingFace](https://huggingface.co/dragonkue)에 공유

### Sentence-Transformers 라이브러리 개선
- **GISTEmbedLoss Multiple Negative 지원** - [PR #2946](https://github.com/UKPLab/sentence-transformers/pull/2946)
- **False Negative 제거 방법 개선** - [PR #3299](https://github.com/UKPLab/sentence-transformers/pull/3299)
  - 절대값/백분율 기준 margin 전략으로 학습 안정성 및 성능 향상
  - 배치 사이즈가 클수록 더 높은 성능 향상 확인

## 주요 성과

### 연구 논문
- 에세이 창의성 점수 예측 모델을 위한 학습데이터 구축 방안 (KCI, 2023.10)
- 에세이 평가 자동화를 위한 대조 학습 기반 다중계층 BERT 모델 손실함수에 관한 연구 (한국정보기술학회논문지, 2023.09)
- CLES-BERT: 에세이 점수 예측을 위한 대조학습 기반 BERT 모델 (한국정보기술학회논문지, 2023.04)
- 자연어처리를 위한 대조 학습 기반의 심층학습 모델에 대한 최근 연구 동향 (대학생논문경진대회, 2022.12)

### 특허
- 2025년 2개 특허 출원 예정

### 커뮤니티 활동

- 인스트럭트-한국 2025 1월 Meetup 발표

**날짜**: 2025.01.25

**개요**: 한국어 LLM 벤치마크인 LogicKor 벤치마크를 운영하는 Instruct KR 주최로 LLM과 RAG 등의 자유 주제 발표에 참여. 허깅페이스에 올린 Embedding Benchmark SOTA 모델을 만든 방법론과 기술적 인사이트를 공유했습니다.

---

이 사이트에서는 제가 진행하고 있는 **오픈소스 활동**, **연구 내용**, **기술 블로그**, 그리고 **학습 여정**을 공유합니다.
