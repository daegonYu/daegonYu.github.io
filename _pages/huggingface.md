---
layout: archive
title: "HuggingFace"
permalink: /huggingface/
author_profile: true
---

## ì˜¤í”ˆì†ŒìŠ¤ í™œë™

AutoRAG Embedding Benchmarkì—ì„œ Bi-Encoderì™€ Cross-Encoder ëª¨ë‘ SOTA ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëª¨ë¸ì„ ë§Œë“¤ê³  ê³µìœ í•˜ëŠ” ê²ƒì„ ì‹œì‘ìœ¼ë¡œ ìµœê·¼ì—ëŠ” **MTEB-ko-retrieval Leaderboard(8ê°€ì§€ í•œêµ­ì–´ Retrieval ë²¤ì¹˜ë§ˆí¬)**ì— ëŒ€í•´ SOTA ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëª¨ë¸ì„ ê³µìœ í–ˆìŠµë‹ˆë‹¤.

<div class="notice--info">
  <h4>HuggingFace Profile</h4>
  <p><a href="https://huggingface.co/dragonkue" target="_blank">ğŸ¤— https://huggingface.co/dragonkue</a></p>
</div>

---

## ì£¼ìš” ëª¨ë¸

### 1. snowflake-arctic-embed-l-v2.0-ko
<div class="model-card">
  <p><strong>ğŸ† SOTA Model</strong></p>
  <p><strong>Link</strong>: <a href="https://huggingface.co/dragonkue/snowflake-arctic-embed-l-v2.0-ko" target="_blank">snowflake-arctic-embed-l-v2.0-ko</a></p>
  <p><strong>Description</strong>: ì„ë² ë”© ëª¨ë¸ì—ì„œ í•œêµ­ì–´ IR ë²¤ì¹˜ë§ˆí¬(<strong>MTEB-ko-retrieval Leaderboard</strong>) ì¢…í•© SOTA ëª¨ë¸</p>
</div>

### 2. BGE-m3-ko
<div class="model-card">
  <p><strong>ğŸ¯ AutoRAG Champion</strong></p>
  <p><strong>Link</strong>: <a href="https://huggingface.co/dragonkue/BGE-m3-ko" target="_blank">BGE-m3-ko</a></p>
  <p><strong>Description</strong>: ì„ë² ë”© ëª¨ë¸ì—ì„œ AutoRAG Embedding ë²¤ì¹˜ë§ˆí¬ì—ì„œ SOTA ëª¨ë¸</p>
</div>

### 3. bge-reranker-v2-m3-ko
<div class="model-card">
  <p><strong>ğŸ”„ Reranker SOTA</strong></p>
  <p><strong>Link</strong>: <a href="https://huggingface.co/dragonkue/bge-reranker-v2-m3-ko" target="_blank">bge-reranker-v2-m3-ko</a></p>
  <p><strong>Description</strong>: Reranker ëª¨ë¸ì—ì„œ AutoRAG Embedding ë²¤ì¹˜ë§ˆí¬ì—ì„œ SOTA ëª¨ë¸</p>
</div>

---

## GitHub ê¸°ì—¬

### sentence-transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°œì„ 

**GISTEmbedLoss ë„¤ê±°í‹°ë¸Œ í™•ì¥ ê¸°ì—¬**ë¥¼ í†µí•´ ì˜¤í”ˆì†ŒìŠ¤ ìƒíƒœê³„ì— ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.

#### ğŸ¯ GISTEmbedLoss 1ì°¨ ê°œì„  (Multiple Negative ì§€ì›):
- **ê¸°ì¡´ ë¬¸ì œ**: GISTEmbedLossëŠ” [Anchor, Positive, Negative] í˜•íƒœì˜ 1:1:1 ìŒë§Œ í•™ìŠµ ê°€ëŠ¥
- **í•´ê²°ì±…**: ìµœê·¼ Bi-Encoder ëª¨ë¸ë“¤(E5, BGE, Arctic ë“±)ì´ **ë³µìˆ˜ì˜ í•˜ë“œ ë„¤ê±°í‹°ë¸Œ(7~15ê°œ)**ë¥¼ í™œìš©í•˜ëŠ” ì¶”ì„¸ë¥¼ ë°˜ì˜
- **êµ¬í˜„**: **Multiple Negative êµ¬ì¡°ë¥¼ ì§€ì›í•˜ë„ë¡ GISTEmbedLoss ì†ì‹¤ í•¨ìˆ˜ êµ¬ì¡° ê°œì„ **
- **ê¸°ìˆ ì  ì˜ì˜**: Guide ëª¨ë¸ì„ í†µí•œ ê¸°ì¡´ MNR Loss ëŒ€ë¹„ **ì•ˆì •ì ì´ê³  íš¨ê³¼ì **ì¸ ì†ì‹¤í•¨ìˆ˜

<div class="notice--success">
  <h4>GitHub Contribution #1</h4>
  <p><a href="https://github.com/UKPLab/sentence-transformers/pull/2946" target="_blank">ğŸ”— Pull Request #2946</a></p>
</div>

#### ğŸ› ï¸ GISTEmbedLoss 2ì°¨ ê°œì„  (NV-Retriever ê¸°ë°˜ Margin Filtering):
- **NV-Retriever ë…¼ë¬¸ ì°¸ê³ **: **false negative í•„í„°ë§ì„ ìœ„í•œ margin ê¸°ë°˜ ë¡œì§**ì„ GISTEmbedLossì— ë„ì…
- **ì°¨ë³„ì **: anchorì— ëŒ€í•œ hard negative ì¶”ì¶œ ì‹œê°€ ì•„ë‹Œ **In-batch negativeì—ì„œ false negative ì œê±° ë°©ë²•ì„ ê°œì„ **
- **ì ˆëŒ€ê°’/ë°±ë¶„ìœ¨ ê¸°ì¤€ margin ì „ëµ**ìœ¼ë¡œ false negative ìƒ˜í”Œ ì œê±° â†’ í•™ìŠµ ì•ˆì •ì„± ë° ì„±ëŠ¥ í–¥ìƒ
- **ë°°ì¹˜ ì‚¬ì´ì¦ˆ íš¨ê³¼**: ë°°ì¹˜ ì‚¬ì´ì¦ˆê°€ í´ìˆ˜ë¡ ë” ë†’ì€ ì„±ëŠ¥ í–¥ìƒ í™•ì¸
- **ì„±ëŠ¥ ê°œì„ **: ê¸°ì¡´ `CachedGISTEmbedLoss` ëŒ€ë¹„ í–¥ìƒëœ ì •í™•ë„(Cosine Accuracy) ë‹¬ì„±

<div class="notice--success">
  <h4>GitHub Contribution #2</h4>
  <p><a href="https://github.com/UKPLab/sentence-transformers/pull/3299" target="_blank">ğŸ”— Pull Request #3299</a></p>
</div>

---

## ì»¤ë®¤ë‹ˆí‹° í™œë™

### ì¸ìŠ¤íŠ¸ëŸ­íŠ¸-í•œêµ­ 2025 1ì›” Meetup ë°œí‘œ

**ë‚ ì§œ**: 2025.01.25

**ê°œìš”**: í•œêµ­ì–´ LLM ë²¤ì¹˜ë§ˆí¬ì¸ LogicKor ë²¤ì¹˜ë§ˆí¬ë¥¼ ìš´ì˜í•˜ëŠ” Instruct KR ì£¼ìµœë¡œ LLMê³¼ RAG ë“±ì˜ ììœ  ì£¼ì œ ë°œí‘œì— ì°¸ì—¬. í—ˆê¹…í˜ì´ìŠ¤ì— ì˜¬ë¦° Embedding Benchmark SOTA ëª¨ë¸ì„ ë§Œë“  ë°©ë²•ë¡ ê³¼ ê¸°ìˆ ì  ì¸ì‚¬ì´íŠ¸ë¥¼ ê³µìœ í–ˆìŠµë‹ˆë‹¤.

ì´ ê¸°ì—¬ë¥¼ í†µí•´ ë” íš¨ê³¼ì ì¸ ì„ë² ë”© ëª¨ë¸ í•™ìŠµì´ ê°€ëŠ¥í•´ì ¸ í•œêµ­ì–´ NLP ì—°êµ¬ ë°œì „ì— ê¸°ì—¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.