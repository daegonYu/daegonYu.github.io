---
layout: archive
title: "Mathematical Foundations"
permalink: /math/
author_profile: true
---

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

## ğŸ§® ë¨¸ì‹ ëŸ¬ë‹ ìˆ˜í•™ ê°œë… ì •ë¦¬

NLPì™€ LLM ì—°êµ¬ì—ì„œ í•µì‹¬ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ìˆ˜í•™ì  ê°œë…ë“¤ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.

---

## 1. InfoNCE Loss

**Contrastive Learning**ì˜ í•µì‹¬ ì†ì‹¤ í•¨ìˆ˜ë¡œ, anchor ìƒ˜í”Œê³¼ positive/negative ì˜ˆì‹œë“¤ì„ ë¹„êµí•©ë‹ˆë‹¤.

$$\mathcal{L}_{\text{InfoNCE}} = -\log \frac{\exp(\text{sim}(a, p) / \tau)}{\exp(\text{sim}(a, p) / \tau) + \sum_{i=1}^{N} \exp(\text{sim}(a, n_i) / \tau)}$$

**íŒŒë¼ë¯¸í„° ì„¤ëª…**:
- $a$: anchor sample (ê¸°ì¤€ ìƒ˜í”Œ)
- $p$: positive sample (ê¸ì • ìƒ˜í”Œ)
- $n_i$: negative samples (ë¶€ì • ìƒ˜í”Œë“¤)
- $\tau$: temperature parameter (ì˜¨ë„ íŒŒë¼ë¯¸í„°)
- $\text{sim}(\cdot, \cdot)$: cosine similarity (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)

**í•µì‹¬ ì•„ì´ë””ì–´**: positive pairì˜ ìœ ì‚¬ë„ëŠ” ë†’ì´ê³ , negative pairì˜ ìœ ì‚¬ë„ëŠ” ë‚®ì¶˜ë‹¤.

---

## 2. False Negative Mitigation

**ì˜ëª» ë¶„ë¥˜ëœ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œ**ì˜ ì•…ì˜í–¥ì„ ì¤„ì´ëŠ” í•µì‹¬ ê¸°ë²•ì…ë‹ˆë‹¤.

$$w_i = \begin{cases}
\alpha \text{ (where } 0 < \alpha < 1 \text{)} & \text{if } \text{sim}(a, n_i) > \theta \\
1 & \text{otherwise}
\end{cases}$$

**ì ìš© ë°©ë²•**:
- **Threshold ê¸°ë°˜**: ìœ ì‚¬ë„ê°€ ë†’ì€ negative sampleì˜ ê°€ì¤‘ì¹˜ ê°ì†Œ
- **Margin ì „ëµ**: ì ˆëŒ€ê°’/ë°±ë¶„ìœ¨ ê¸°ì¤€ìœ¼ë¡œ false negative ì œê±°
- **ë°°ì¹˜ í¬ê¸° íš¨ê³¼**: í° ë°°ì¹˜ì—ì„œ ë” íš¨ê³¼ì 

---

## 3. Hard Negative Mining

**ì–´ë ¤ìš´ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œ**ì„ ì„ ë³„í•˜ì—¬ í•™ìŠµ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤.

### Distance-based Filtering
$$\text{HardNeg} = \{n_i \mid \text{sim}(a, n_i) > \text{threshold}\}$$

### Semi-Hard Negative Selection
$$\text{SemiHardNeg} = \{n_i \mid \text{sim}(a, p) < \text{sim}(a, n_i) < \text{sim}(a, p) + \text{margin}\}$$

**Curriculum Learning**: ì ì§„ì ìœ¼ë¡œ ì–´ë ¤ìš´ ìƒ˜í”Œë“¤ì„ í•™ìŠµì— ë„ì…

---

## 4. Dual-Encoder Retrieval

**ë…ë¦½ì ì¸ ì¸ì½”ë”**ë¥¼ ì‚¬ìš©í•œ íš¨ìœ¨ì  ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¡°ì…ë‹ˆë‹¤.

$$\text{score}(q, d) = f_q(q)^T \cdot f_d(d)$$

**ì¥ì **:
- $f_q(\cdot)$: query encoder (ì¿¼ë¦¬ ì „ìš©)
- $f_d(\cdot)$: document encoder (ë¬¸ì„œ ì „ìš©)
- **ë³‘ë ¬ ì²˜ë¦¬**: ë¬¸ì„œë“¤ì„ ë¯¸ë¦¬ ì¸ì½”ë”©í•˜ì—¬ ë¹ ë¥¸ ê²€ìƒ‰ ê°€ëŠ¥

---

## 5. LLM Training Loss Functions

### 5.1. Supervised Fine-Tuning (SFT)

**ê¸°ë³¸ì ì¸ ì–¸ì–´ëª¨ë¸ íŒŒì¸íŠœë‹**ì„ ìœ„í•œ ì†ì‹¤í•¨ìˆ˜ì…ë‹ˆë‹¤.

$$\mathcal{L}_{\text{SFT}} = -\sum_{t=1}^{T} \log p_\theta(y_t | x, y_{<t})$$

**íŠ¹ì§•**:
- $x$: input prompt (ì…ë ¥ í”„ë¡¬í”„íŠ¸)
- $y_t$: target token at time $t$ (ì‹œê°„ $t$ì˜ íƒ€ê²Ÿ í† í°)
- **Next Token Prediction**: ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” í‘œì¤€ ì–¸ì–´ëª¨ë¸ë§

### 5.2. Direct Preference Optimization (DPO)

**Direct Preference Optimization**ì„ í†µí•œ ì¸ê°„ ì„ í˜¸ë„ í•™ìŠµì…ë‹ˆë‹¤.

$$\mathcal{L}_{\text{DPO}} = -\mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right]$$

**í•µì‹¬ êµ¬ì„±ìš”ì†Œ**:
- $y_w$: preferred response (ì„ í˜¸ ì‘ë‹µ)
- $y_l$: non-preferred response (ë¹„ì„ í˜¸ ì‘ë‹µ)
- $\pi_\theta$: target model (í•™ìŠµ ëª¨ë¸)
- $\pi_{\text{ref}}$: reference model (ê¸°ì¤€ ëª¨ë¸)
- $\beta$: scaling parameter (ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°)

### 5.3. Identity Preference Optimization (IPO)

**DPOì˜ ê°œì„ ëœ ë²„ì „**ìœ¼ë¡œ, ë” ì•ˆì •ì ì¸ í•™ìŠµì„ ì œê³µí•©ë‹ˆë‹¤.

$$\mathcal{L}_{\text{IPO}} = \mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} \left[ \left( \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} - \frac{1}{\beta} \right)^2 \right]$$

**ì¥ì **:
- **ì•ˆì •ì„±**: ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ì—†ì´ ì œê³± ì†ì‹¤ ì‚¬ìš©
- **ìˆ˜ë ´ì„±**: DPO ëŒ€ë¹„ ë” ì•ˆì •ì ì¸ ìˆ˜ë ´ íŠ¹ì„±

### 5.4. Group Relative Policy Optimization (GRPO)

**ê·¸ë£¹ ê¸°ë°˜ ìƒëŒ€ì  ì •ì±… ìµœì í™”**ë¡œ ë‹¤ì¤‘ ì‘ë‹µì„ ë™ì‹œì— ê³ ë ¤í•©ë‹ˆë‹¤.

$$\mathcal{L}_{\text{GRPO}} = -\mathbb{E}_{(x,\{y_i\}) \sim \mathcal{D}} \left[ \sum_{i=1}^{n} r_i \log \frac{\pi_\theta(y_i|x)}{\pi_{\text{ref}}(y_i|x)} \right]$$

**íŠ¹ì§•**:
- $\{y_i\}$: multiple responses from the same prompt (ë™ì¼ í”„ë¡¬í”„íŠ¸ì˜ ë‹¤ì¤‘ ì‘ë‹µ)
- $r_i$: relative reward for response $i$ (ì‘ë‹µ $i$ì˜ ìƒëŒ€ì  ë³´ìƒ)
- **Group Learning**: ì—¬ëŸ¬ ì‘ë‹µì„ ë™ì‹œì— ê³ ë ¤í•œ í•™ìŠµ

---

## 6. GISTEmbedLoss (ë‚˜ì˜ ê¸°ì—¬)

**Guide ëª¨ë¸**ì„ í™œìš©í•œ ê°œì„ ëœ ì„ë² ë”© í•™ìŠµ ì†ì‹¤ í•¨ìˆ˜ì…ë‹ˆë‹¤.

$$\mathcal{L}_{\text{GIST}} = \mathcal{L}_{\text{InfoNCE}} + \lambda \cdot \mathcal{L}_{\text{Guide}}$$

**ë‚˜ì˜ ê°œì„ ì‚¬í•­**:
- **Multiple Negative ì§€ì›**: ê¸°ì¡´ 1:1:1 êµ¬ì¡°ë¥¼ 1:1:Nìœ¼ë¡œ í™•ì¥
- **False Negative ì™„í™”**: In-batch negativeì—ì„œ false negative ì œê±°
- **ì•ˆì •ì„± í–¥ìƒ**: ê°€ì´ë“œ ëª¨ë¸ì˜ ì§€ì‹ ì¦ë¥˜ë¡œ í•™ìŠµ ì•ˆì •í™”

$$\mathcal{L}_{\text{Guide}} = \text{KL}(\text{Guide}(a, p, \{n_i\}) \parallel \text{Student}(a, p, \{n_i\}))$$

---

## 7. Attention Mechanism

**Transformerì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜**ì…ë‹ˆë‹¤.

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**Multi-Head Attention**:
$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O$$

ì—¬ê¸°ì„œ ê° $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$

---

## ğŸ¯ ìˆ˜ì‹ ë°ëª¨

ê°„ë‹¨í•œ ì˜ˆì œë¡œ **ì½”ì‚¬ì¸ ìœ ì‚¬ë„** ê³„ì‚°:

ë‘ ë²¡í„° $\mathbf{a} = [1, 2, 3]$, $\mathbf{b} = [4, 5, 6]$ì— ëŒ€í•´:

$$\text{cosine\_similarity}(\mathbf{a}, \mathbf{b}) = \frac{\mathbf{a} \cdot \mathbf{b}}{|\mathbf{a}| \cdot |\mathbf{b}|} = \frac{32}{\sqrt{14} \cdot \sqrt{77}} \approx 0.974$$

ì´ ìˆ˜í•™ì  ê¸°ì´ˆë“¤ì´ ì œê°€ ê°œë°œí•œ SOTA ì„ë² ë”© ëª¨ë¸ë“¤ì˜ í•µì‹¬ì´ ë˜ì—ˆìŠµë‹ˆë‹¤! ğŸš€