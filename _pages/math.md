---
layout: archive
title: "Math"
permalink: /math/
author_profile: true
---

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true
  }
});
</script>

## ğŸ§® ë¨¸ì‹ ëŸ¬ë‹ ìˆ˜í•™ ê°œë… ì •ë¦¬

NLPì™€ LLM ì—°êµ¬ì—ì„œ í•µì‹¬ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ìˆ˜í•™ì  ê°œë…ë“¤ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.

---

## 1. InfoNCE Loss

**Contrastive Learning**ì˜ í•µì‹¬ ì†ì‹¤ í•¨ìˆ˜ë¡œ, anchor ìƒ˜í”Œê³¼ positive/negative ì˜ˆì‹œë“¤ì„ ë¹„êµí•©ë‹ˆë‹¤.

$$\mathcal{L}_{\text{InfoNCE}} = -\log \frac{\exp(\text{sim}(a, p) / \tau)}{\exp(\text{sim}(a, p) / \tau) + \sum_{i=1}^{N} \exp(\text{sim}(a, n_i) / \tau)}$$

**íŒŒë¼ë¯¸í„° ì„¤ëª…**:
- $a$: anchor sample (ê¸°ì¤€ ìƒ˜í”Œ)
- $p$: positive sample (ê¸ì • ìƒ˜í”Œ)
- $n_i$: negative samples (ë¶€ì • ìƒ˜í”Œë“¤)
- $\tau$: temperature parameter (ì˜¨ë„ íŒŒë¼ë¯¸í„°)
- $\text{sim}(\cdot, \cdot)$: cosine similarity (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)

**í•µì‹¬ ì•„ì´ë””ì–´**: positive pairì˜ ìœ ì‚¬ë„ëŠ” ë†’ì´ê³ , negative pairì˜ ìœ ì‚¬ë„ëŠ” ë‚®ì¶˜ë‹¤.

---

## 2. False Negative Mitigation

**ì˜ëª» ë¶„ë¥˜ëœ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œ**ì˜ ì•…ì˜í–¥ì„ ì¤„ì´ëŠ” í•µì‹¬ ê¸°ë²•ì…ë‹ˆë‹¤.

$$w_i = \begin{cases}
\alpha \text{ (where } 0 < \alpha < 1 \text{)} & \text{if } \text{sim}(a, n_i) > \theta \\
1 & \text{otherwise}
\end{cases}$$

**ì ìš© ë°©ë²•**:
- **Threshold ê¸°ë°˜**: ìœ ì‚¬ë„ê°€ ë†’ì€ negative sampleì˜ ê°€ì¤‘ì¹˜ ê°ì†Œ
- **Margin ì „ëµ**: ì ˆëŒ€ê°’/ë°±ë¶„ìœ¨ ê¸°ì¤€ìœ¼ë¡œ false negative ì œê±°
- **ë°°ì¹˜ í¬ê¸° íš¨ê³¼**: í° ë°°ì¹˜ì—ì„œ ë” íš¨ê³¼ì 

---

## 3. Hard Negative Mining

**ì–´ë ¤ìš´ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œ**ì„ ì„ ë³„í•˜ì—¬ í•™ìŠµ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤.

### Distance-based Filtering
$$\text{HardNeg} = \{n_i \mid \text{sim}(a, n_i) > \text{threshold}\}$$

### Semi-Hard Negative Selection
$$\text{SemiHardNeg} = \{n_i \mid \text{sim}(a, p) < \text{sim}(a, n_i) < \text{sim}(a, p) + \text{margin}\}$$

**Curriculum Learning**: ì ì§„ì ìœ¼ë¡œ ì–´ë ¤ìš´ ìƒ˜í”Œë“¤ì„ í•™ìŠµì— ë„ì…

---

## 4. Dual-Encoder Retrieval

**ë…ë¦½ì ì¸ ì¸ì½”ë”**ë¥¼ ì‚¬ìš©í•œ íš¨ìœ¨ì  ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¡°ì…ë‹ˆë‹¤.

$$\text{score}(q, d) = f_q(q)^T \cdot f_d(d)$$

**ì¥ì **:
- $f_q(\cdot)$: query encoder (ì¿¼ë¦¬ ì „ìš©)
- $f_d(\cdot)$: document encoder (ë¬¸ì„œ ì „ìš©)
- **ë³‘ë ¬ ì²˜ë¦¬**: ë¬¸ì„œë“¤ì„ ë¯¸ë¦¬ ì¸ì½”ë”©í•˜ì—¬ ë¹ ë¥¸ ê²€ìƒ‰ ê°€ëŠ¥

---

## 5. RLHFì˜ DPO Loss

**Direct Preference Optimization**ì„ í†µí•œ ì¸ê°„ ì„ í˜¸ë„ í•™ìŠµì…ë‹ˆë‹¤.

$$\mathcal{L}_{\text{DPO}} = -\mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right]$$

**í•µì‹¬ êµ¬ì„±ìš”ì†Œ**:
- $y_w$: preferred response (ì„ í˜¸ ì‘ë‹µ)
- $y_l$: non-preferred response (ë¹„ì„ í˜¸ ì‘ë‹µ)
- $\pi_\theta$: target model (í•™ìŠµ ëª¨ë¸)
- $\pi_{\text{ref}}$: reference model (ê¸°ì¤€ ëª¨ë¸)
- $\beta$: scaling parameter (ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°)
- $\sigma$: sigmoid function (ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜)

---

## 6. GISTEmbedLoss (ë‚˜ì˜ ê¸°ì—¬)

**Guide ëª¨ë¸**ì„ í™œìš©í•œ ê°œì„ ëœ ì„ë² ë”© í•™ìŠµ ì†ì‹¤ í•¨ìˆ˜ì…ë‹ˆë‹¤.

$$\mathcal{L}_{\text{GIST}} = \mathcal{L}_{\text{InfoNCE}} + \lambda \cdot \mathcal{L}_{\text{Guide}}$$

**ë‚˜ì˜ ê°œì„ ì‚¬í•­**:
- **Multiple Negative ì§€ì›**: ê¸°ì¡´ 1:1:1 êµ¬ì¡°ë¥¼ 1:1:Nìœ¼ë¡œ í™•ì¥
- **False Negative ì™„í™”**: In-batch negativeì—ì„œ false negative ì œê±°
- **ì•ˆì •ì„± í–¥ìƒ**: ê°€ì´ë“œ ëª¨ë¸ì˜ ì§€ì‹ ì¦ë¥˜ë¡œ í•™ìŠµ ì•ˆì •í™”

$$\mathcal{L}_{\text{Guide}} = \text{KL}(\text{Guide}(a, p, \{n_i\}) \parallel \text{Student}(a, p, \{n_i\}))$$

---

## 7. Attention Mechanism

**Transformerì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜**ì…ë‹ˆë‹¤.

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**Multi-Head Attention**:
$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O$$

ì—¬ê¸°ì„œ ê° $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$

---

## ğŸ¯ ìˆ˜ì‹ ë°ëª¨

ê°„ë‹¨í•œ ì˜ˆì œë¡œ **ì½”ì‚¬ì¸ ìœ ì‚¬ë„** ê³„ì‚°:

ë‘ ë²¡í„° $\mathbf{a} = [1, 2, 3]$, $\mathbf{b} = [4, 5, 6]$ì— ëŒ€í•´:

$$\text{cosine\_similarity}(\mathbf{a}, \mathbf{b}) = \frac{\mathbf{a} \cdot \mathbf{b}}{|\mathbf{a}| \cdot |\mathbf{b}|} = \frac{32}{\sqrt{14} \cdot \sqrt{77}} \approx 0.974$$

ì´ ìˆ˜í•™ì  ê¸°ì´ˆë“¤ì´ ì œê°€ ê°œë°œí•œ SOTA ì„ë² ë”© ëª¨ë¸ë“¤ì˜ í•µì‹¬ì´ ë˜ì—ˆìŠµë‹ˆë‹¤! ğŸš€