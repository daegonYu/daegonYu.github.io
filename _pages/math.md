---
layout: archive
title: "Mathematical Foundations"
permalink: /math/
author_profile: true
---

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

## ğŸ§® ë¨¸ì‹ ëŸ¬ë‹ ìˆ˜í•™ ê°œë… ì •ë¦¬

NLPì™€ LLM ì—°êµ¬ì—ì„œ í•µì‹¬ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ìˆ˜í•™ì  ê°œë…ë“¤ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.

---

## InfoNCE Loss

**Contrastive Learning**ì˜ í•µì‹¬ ì†ì‹¤ í•¨ìˆ˜ë¡œ, anchor ìƒ˜í”Œê³¼ positive/negative ì˜ˆì‹œë“¤ì„ ë¹„êµí•©ë‹ˆë‹¤.

$$\mathcal{L}_{\text{InfoNCE}} = -\log \frac{\exp(\text{sim}(a, p) / \tau)}{\exp(\text{sim}(a, p) / \tau) + \sum_{i=1}^{N} \exp(\text{sim}(a, n_i) / \tau)}$$

**íŒŒë¼ë¯¸í„° ì„¤ëª…**:
- $a$: anchor sample (ê¸°ì¤€ ìƒ˜í”Œ)
- $p$: positive sample (ê¸ì • ìƒ˜í”Œ)
- $n_i$: negative samples (ë¶€ì • ìƒ˜í”Œë“¤)
- $\tau$: temperature parameter (ì˜¨ë„ íŒŒë¼ë¯¸í„°)
- $\text{sim}(\cdot, \cdot)$: cosine similarity (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)

**í•µì‹¬ ì•„ì´ë””ì–´**: positive pairì˜ ìœ ì‚¬ë„ëŠ” ë†’ì´ê³ , negative pairì˜ ìœ ì‚¬ë„ëŠ” ë‚®ì¶˜ë‹¤.

---

## Hard Negative Filtering

**ì–´ë ¤ìš´ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œ**ì„ í•„í„°ë§í•˜ì—¬ í•™ìŠµ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤.

### Semi-Hard Negative Selection
$$\text{SemiHardNeg} = \{n_i \mid \text{sim}(a, p) - \text{margin} < \text{sim}(a, n_i)$$

---

## 4. Dual-Encoder Retrieval

**ë…ë¦½ì ì¸ ì¸ì½”ë”**ë¥¼ ì‚¬ìš©í•œ íš¨ìœ¨ì  ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¡°ì…ë‹ˆë‹¤.

$$\text{score}(q, d) = f_q(q)^T \cdot f_d(d)$$

**ì¥ì **:
- $f_q(\cdot)$: query encoder (ì¿¼ë¦¬ ì „ìš©)
- $f_d(\cdot)$: document encoder (ë¬¸ì„œ ì „ìš©)
- **ë³‘ë ¬ ì²˜ë¦¬**: ë¬¸ì„œë“¤ì„ ë¯¸ë¦¬ ì¸ì½”ë”©í•˜ì—¬ ë¹ ë¥¸ ê²€ìƒ‰ ê°€ëŠ¥

---

## 5. LLM Training Loss Functions

### 5.1. Supervised Fine-Tuning (SFT)

**ê¸°ë³¸ì ì¸ ì–¸ì–´ëª¨ë¸ íŒŒì¸íŠœë‹**ì„ ìœ„í•œ ì†ì‹¤í•¨ìˆ˜ì…ë‹ˆë‹¤.

$$\mathcal{L}_{\text{SFT}} = -\sum_{t=1}^{T} \log p_\theta(y_t | x, y_{<t})$$

**íŠ¹ì§•**:
- $x$: input prompt (ì…ë ¥ í”„ë¡¬í”„íŠ¸)
- $y_t$: target token at time $t$ (ì‹œê°„ $t$ì˜ íƒ€ê²Ÿ í† í°)
- **Next Token Prediction**: ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” í‘œì¤€ ì–¸ì–´ëª¨ë¸ë§

### 5.2. Direct Preference Optimization (DPO)

**Direct Preference Optimization**ì„ í†µí•œ ì¸ê°„ ì„ í˜¸ë„ í•™ìŠµì…ë‹ˆë‹¤.

$$\mathcal{L}_{\text{DPO}} = -\mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right]$$

**í•µì‹¬ êµ¬ì„±ìš”ì†Œ**:
- $y_w$: preferred response (ì„ í˜¸ ì‘ë‹µ)
- $y_l$: non-preferred response (ë¹„ì„ í˜¸ ì‘ë‹µ)
- $\pi_\theta$: target model (í•™ìŠµ ëª¨ë¸)
- $\pi_{\text{ref}}$: reference model (ê¸°ì¤€ ëª¨ë¸)
- $\beta$: scaling parameter (ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°)

### 5.3. Identity Preference Optimization (IPO)

**DPOì˜ ê°œì„ ëœ ë²„ì „**ìœ¼ë¡œ, ë” ì•ˆì •ì ì¸ í•™ìŠµì„ ì œê³µí•©ë‹ˆë‹¤.

$$\mathcal{L}_{\text{IPO}} = \mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} \left[ \left( \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} - \frac{1}{\beta} \right)^2 \right]$$

**ì¥ì **:
- **ì•ˆì •ì„±**: ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ì—†ì´ ì œê³± ì†ì‹¤ ì‚¬ìš©
- **ìˆ˜ë ´ì„±**: DPO ëŒ€ë¹„ ë” ì•ˆì •ì ì¸ ìˆ˜ë ´ íŠ¹ì„±

### 5.4. Group Relative Policy Optimization (GRPO)

**ê·¸ë£¹ ê¸°ë°˜ ìƒëŒ€ì  ì •ì±… ìµœì í™”**ë¡œ ë‹¤ì¤‘ ì‘ë‹µì„ ë™ì‹œì— ê³ ë ¤í•©ë‹ˆë‹¤.

$$\mathcal{L}_{\text{GRPO}} = -\mathbb{E}_{(x,\{y_i\}) \sim \mathcal{D}} \left[ \sum_{i=1}^{n} r_i \log \frac{\pi_\theta(y_i|x)}{\pi_{\text{ref}}(y_i|x)} \right]$$

**íŠ¹ì§•**:
- $\{y_i\}$: multiple responses from the same prompt (ë™ì¼ í”„ë¡¬í”„íŠ¸ì˜ ë‹¤ì¤‘ ì‘ë‹µ)
- $r_i$: relative reward for response $i$ (ì‘ë‹µ $i$ì˜ ìƒëŒ€ì  ë³´ìƒ)
- **Group Learning**: ì—¬ëŸ¬ ì‘ë‹µì„ ë™ì‹œì— ê³ ë ¤í•œ í•™ìŠµ

---

